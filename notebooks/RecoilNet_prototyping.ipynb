{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (RecoilNet.py, line 48)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/brian/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5efc3c4209f1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from RecoilNet import RecoilNet\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/brian/Desktop/fun_stuff/fortnight-furniture/notebooks/RecoilNet.py\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    x = layers.LeakyReLU()x)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from RecoilNet import RecoilNet\n",
    "from plot_model import plot_hist\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # data_link_dict = get_skfold_data(path=\"../data/imgs/*.jpg\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # decommisioned because inflight data augmentation solves a lot of these\n",
    "    # problems\n",
    "\n",
    "    # Use json to load the permanent dictionary that has been Created\n",
    "#     with open(\"../data/data_splits.json\") as infile:\n",
    "#         data_link_dict = json.load(infile)\n",
    "\n",
    "    EPOCHS = 10\n",
    "    AUGMENTATION = 1    # could do 3 epochs of 10 augmentation or 30 of 1 which\n",
    "                        # provides more data for plots to work with\n",
    "\n",
    "    MINITRAINS = 10\n",
    "    DO = 0.55  # drop out\n",
    "\n",
    "    # for Adam inital LR of 0.0001 is a good starting point\n",
    "    # for SGD initial LR of 0.001 is a good starting point\n",
    "    LR = 0.0005\n",
    "    DECAY = 0.5e-6\n",
    "    L2_REG = 0.01\n",
    "    OPTIMIZER = Adam(lr=LR, decay=DECAY)\n",
    "    # OPTIMIZER = SGD(lr=LR, momentum=0.9, nesterov=True)\n",
    "\n",
    "    MODEL_ID = 'v3_0a'\n",
    "\n",
    "    plot_file = \"model_{:}.png\".format(MODEL_ID)\n",
    "    weights_file = \"weights/model_{:}_weights.h5\".format(MODEL_ID)\n",
    "    history_file = \"histories/history_{:}.json\".format(MODEL_ID)\n",
    "\n",
    "    # # user parameters for LoaderBot v1.0\n",
    "    # # Parameters for Generators\n",
    "    # params = {'dim': (299, 299),\n",
    "    #           'batch_size': 256,\n",
    "    #           'n_classes': 128,\n",
    "    #           'n_channels': 3,\n",
    "    #           'shuffle': False}\n",
    "\n",
    "    # These parameters are for LoaderBot v2.0\n",
    "    # Parameters for Generators\n",
    "    params = {'dim': (299, 299),\n",
    "              'batch_size': 128,\n",
    "              'n_classes': 128,\n",
    "              'n_channels': 3,\n",
    "              'augmentation': AUGMENTATION,\n",
    "              'shuffle': True}\n",
    "    #\n",
    "    # Parameters for Generators\n",
    "    test_params = {'dim': (299, 299),\n",
    "                   'batch_size': 128,\n",
    "                   'n_classes': 128,\n",
    "                   'n_channels': 3,\n",
    "                   'augmentation': 1,\n",
    "                   'augment': False,\n",
    "                   'shuffle': False}\n",
    "\n",
    "#     # Datasets\n",
    "#     X_train_img_paths = data_link_dict[\"X_test_1\"]\n",
    "#     y_train = data_link_dict[\"y_test_1\"]\n",
    "\n",
    "#     X_test_img_paths = data_link_dict[\"X_test_2\"]\n",
    "#     y_test = data_link_dict[\"y_test_2\"]\n",
    "\n",
    "#     # Generators\n",
    "#     training_generator = LoaderBot(X_train_img_paths, y_train, **params)\n",
    "#     validation_generator = LoaderBot(X_test_img_paths, y_test, **test_params)\n",
    "\n",
    "    # setup model\n",
    "#     base_model = InceptionV3(weights='imagenet', include_top=False) #include_top=False excludes final FC layer\n",
    "#     model = regular_brian_layers(base_model, 128, DO, l1_reg=0.0001)\n",
    "\n",
    "    model = RecoilNet()\n",
    "    \n",
    "    # print(model.summary())\n",
    "\n",
    "    # mini-train 1, like normal\n",
    "    # transfer learning\n",
    "    setup_to_transfer_learn(model, base_model, OPTIMIZER)\n",
    "\n",
    "    history = {}   # preinitialize history log\n",
    "\n",
    "    for mt in range(MINITRAINS):\n",
    "        temp = mt + 1\n",
    "\n",
    "        if temp == 1:\n",
    "            # Run model\n",
    "            new_history = model.fit_generator(generator=training_generator,\n",
    "                                             validation_data=validation_generator,\n",
    "                                             epochs=EPOCHS,\n",
    "                                             use_multiprocessing=False)\n",
    "\n",
    "            history[\"acc\"] = new_history.history[\"acc\"]\n",
    "            history[\"val_acc\"] = new_history.history[\"val_acc\"]\n",
    "            history[\"loss\"] = new_history.history[\"loss\"]\n",
    "            history[\"val_loss\"] = new_history.history[\"val_loss\"]\n",
    "\n",
    "        else:\n",
    "            # mini-train 2\n",
    "            OPTIMIZER = Adam(lr=LR / 2**temp, decay=DECAY)\n",
    "            # try to fine tune some of the InceptionV3 layers also\n",
    "            setup_to_finetune(model, NB_IV3_LAYERS_TO_FREEZE - (5 * temp), OPTIMIZER, L2_REG)\n",
    "\n",
    "            print(\"\\n\\n        Starting epoch {:}\\n\\n\".format(EPOCHS * mt + 1))\n",
    "\n",
    "            # Run model\n",
    "            new_history = model.fit_generator(generator=training_generator,\n",
    "                                              validation_data=validation_generator,\n",
    "                                              epochs=EPOCHS,\n",
    "                                              use_multiprocessing=False)\n",
    "\n",
    "            # save the weights in case we want to predict on them later\n",
    "            model.save(weights_file)\n",
    "\n",
    "            history[\"acc\"] += new_history.history[\"acc\"]\n",
    "            history[\"val_acc\"] += new_history.history[\"val_acc\"]\n",
    "            history[\"loss\"] += new_history.history[\"loss\"]\n",
    "            history[\"val_loss\"] += new_history.history[\"val_loss\"]\n",
    "\n",
    "        # seems to be prepending a 0 to the list so ignore that\n",
    "        history[\"acc\"] = history[\"acc\"]\n",
    "        history[\"val_acc\"] = history[\"val_acc\"]\n",
    "        history[\"loss\"] = history[\"loss\"]\n",
    "        history[\"val_loss\"] = history[\"val_loss\"]\n",
    "\n",
    "        plot_hist(history, plot_file, epochs=len(history[\"acc\"]), sprint=True)\n",
    "\n",
    "    # try to save the history so models can be more easily compared and Also\n",
    "    # to better log results if going back is needed\n",
    "    with open(history_file, \"w\") as outfile:\n",
    "        json.dump(history, outfile)\n",
    "\n",
    "    print(\"\\n\\n\\n\\nCompleted in {:6.2f} hrs\".format(((time.time() - start_time)) / 3600))  # convert to hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-37ec931a76b6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# data_link_dict = get_skfold_data(path=\"../data/imgs/*.jpg\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# decommisioned because inflight data augmentation solves a lot of these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
